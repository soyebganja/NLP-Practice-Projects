{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwYlxw68lABH+TRQPOLzD8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soyebganja/NLP-Practice-Projects/blob/main/exercise/Spacy_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgbfbiGwG9Ty"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "doc = nlp(\"Caption america ate 100$ of samosa. Then he said I can do this all day.\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itrqTIBLG-uW",
        "outputId": "b51cc700-d18a-4ac8-f671-de7a6eced061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption\n",
            "america\n",
            "ate\n",
            "100\n",
            "$\n",
            "of\n",
            "samosa\n",
            ".\n",
            "Then\n",
            "he\n",
            "said\n",
            "I\n",
            "can\n",
            "do\n",
            "this\n",
            "all\n",
            "day\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40wdNSiZHLR4",
        "outputId": "be7505fe-bad5-47be-cdee-6d0e4893b0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "TOmxsdZ2HmUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLcHl53VIWq0",
        "outputId": "e882583d-c3ae-4eeb-8c2e-5088c74533cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjWNTt0lIYL3",
        "outputId": "cd8e46ae-5619-4c2a-d3db-3f53e1232620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7b64df4fd9d0>),\n",
              " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7b64df4fda30>),\n",
              " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7b65277d5770>),\n",
              " ('attribute_ruler',\n",
              "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7b64df351250>),\n",
              " ('lemmatizer',\n",
              "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7b64df350650>),\n",
              " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7b64df79a8f0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Caption america ate 100$ of samosa. Then he said I can do this all day.\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token, \" | \", spacy.explain(token.pos_), \" | \", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7uY7ZluIlxR",
        "outputId": "11cf1f70-3c19-4d1b-b2d7-8d018f9793c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption  |  proper noun  |  Caption\n",
            "america  |  proper noun  |  america\n",
            "ate  |  verb  |  eat\n",
            "100  |  numeral  |  100\n",
            "$  |  numeral  |  $\n",
            "of  |  adposition  |  of\n",
            "samosa  |  proper noun  |  samosa\n",
            ".  |  punctuation  |  .\n",
            "Then  |  adverb  |  then\n",
            "he  |  pronoun  |  he\n",
            "said  |  verb  |  say\n",
            "I  |  pronoun  |  I\n",
            "can  |  auxiliary  |  can\n",
            "do  |  verb  |  do\n",
            "this  |  pronoun  |  this\n",
            "all  |  determiner  |  all\n",
            "day  |  noun  |  day\n",
            ".  |  punctuation  |  .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBjmbkZqJodp",
        "outputId": "102def76-27a7-44e0-bd6e-846313231e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
            "$45 billion  |  MONEY  |  Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"ent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "7ZLiWHCJJ-xT",
        "outputId": "432af613-399b-4fb8-b484-d08dd6df126d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tesla Inc\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is going to acquire twitter for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $45 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMjPvUsjRwfE",
        "outputId": "2aa74ad0-875a-409f-b089-3382a2fc3bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "doc = nlp(\"Tesla Inc va racheter Twitter pour $45 milliards de dollars\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21H6KZbfQkB_",
        "outputId": "268738c3-8c07-41e8-e175-d64ca7a0de22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc  |  PER  |  Named person or family.\n",
            "Twitter  |  MISC  |  Miscellaneous entities, e.g. events, nationalities, products or works of art\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "doc = nlp(\"Tesla Inc va racheter Twitter pour $45 milliards de dollars\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))\n"
      ],
      "metadata": {
        "id": "mZQqyn7nRqpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "souece_nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "nlp.add_pipe(\"ner\", source=souece_nlp)\n",
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGWpGYljTB4E",
        "outputId": "a04734ee-1efc-473c-9c4d-9babc667d516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ner']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Tesla Inc va racheter Twitter pour $45 milliards de dollars\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdPIFfw0TMto",
        "outputId": "b47d24ed-4f9a-46ba-c7fb-69157ae98a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc va  |  ORG  |  Companies, agencies, institutions, etc.\n",
            "Twitter  |  PERSON  |  People, including fictional\n",
            "$45 milliards de dollars  |  MONEY  |  Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "FJqAOVmaTQ6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
        "\n",
        "for word in words:\n",
        "    print(word, \"|\", stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7UZSIQaX5M_",
        "outputId": "2e343850-6c03-4ded-8624-5cacecf6a279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating | eat\n",
            "eats | eat\n",
            "eat | eat\n",
            "ate | ate\n",
            "adjustable | adjust\n",
            "rafting | raft\n",
            "ability | abil\n",
            "meeting | meet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(\"Eating eats eat ate adjustable rafting ability meeting better\")\n",
        "for token in doc:\n",
        "    print(token, \" | \", token.lemma_, \" | \", token.lemma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Efn9m9qlYAGF",
        "outputId": "325279d5-6a29-475b-aabd-d372736b377f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eating  |  eating  |  12092082220177030354\n",
            "eats  |  eat  |  9837207709914848172\n",
            "eat  |  eat  |  9837207709914848172\n",
            "ate  |  eat  |  9837207709914848172\n",
            "adjustable  |  adjustable  |  6033511944150694480\n",
            "rafting  |  raft  |  7154368781129989833\n",
            "ability  |  ability  |  11565809527369121409\n",
            "meeting  |  meet  |  6880656908171229526\n",
            "better  |  well  |  4525988469032889948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Mando talked for 3 hours although talking isn't his thing he became talkative\")\n",
        "for token in doc:\n",
        "    print(token, \" | \", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqGZ6B11YwMH",
        "outputId": "240f711a-24aa-40c8-b2f1-ab14c3521e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mando  |  Mando\n",
            "talked  |  talk\n",
            "for  |  for\n",
            "3  |  3\n",
            "hours  |  hour\n",
            "although  |  although\n",
            "talking  |  talk\n",
            "is  |  be\n",
            "n't  |  not\n",
            "his  |  his\n",
            "thing  |  thing\n",
            "he  |  he\n",
            "became  |  become\n",
            "talkative  |  talkative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v40n0NI7ZY3Q",
        "outputId": "61bc95c4-62d6-4f10-a382-9d8cb8434a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jGu0ZFZZ7GA",
        "outputId": "305779fd-0f71-4683-d241-7d84c2774c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bro | Bro\n",
            ", | ,\n",
            "you | you\n",
            "wanna | wanna\n",
            "go | go\n",
            "? | ?\n",
            "Brah | Brah\n",
            ", | ,\n",
            "do | do\n",
            "n't | not\n",
            "say | say\n",
            "no | no\n",
            "! | !\n",
            "I | I\n",
            "am | be\n",
            "exhausted | exhaust\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGpyRBjnaHjC",
        "outputId": "f09efc11-365a-472f-e67e-397c6f0addf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Bro"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0].lemma_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oDKlqEMfaa4j",
        "outputId": "540c8603-b64d-4773-8c1e-14a657ee9a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bro'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ar = nlp.get_pipe('attribute_ruler')\n",
        "\n",
        "ar.add([[{\"TEXT\":\"Bro\"}],[{\"TEXT\":\"Brah\"}]],{\"LEMMA\":\"Brother\"})\n",
        "\n",
        "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72N0Brx3aeyK",
        "outputId": "f3bc3303-9ce8-4a80-d749-a9d323640629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bro | Brother\n",
            ", | ,\n",
            "you | you\n",
            "wanna | wanna\n",
            "go | go\n",
            "? | ?\n",
            "Brah | Brother\n",
            ", | ,\n",
            "do | do\n",
            "n't | not\n",
            "say | say\n",
            "no | no\n",
            "! | !\n",
            "I | I\n",
            "am | be\n",
            "exhausted | exhaust\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcU-SP3ua_l8",
        "outputId": "070c8400-677d-4e14-9c8f-1d983e10dffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Bro"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0].lemma_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "emHjToYHbBXZ",
        "outputId": "a8618e43-e6f8-4f82-fdd7-989672d1f0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Brother'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Exercise1:\n",
        "\n",
        "*   Convert these list of words into base form using Stemming and Lemmatization and observe the transformations\n",
        "*   Write a short note on the words that have different base words using stemming and Lemmatization\n",
        "\n"
      ],
      "metadata": {
        "id": "-CRRgwX4eK_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using stemming in nltk\n",
        "lst_words = ['running', 'painting', 'walking', 'dressing', 'likely', 'children', 'whom', 'good', 'ate', 'fishing']\n",
        "\n",
        "for word in lst_words:\n",
        "    print(word, \"|\", stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJjI43L3bEDS",
        "outputId": "504cb268-2e96-4db6-a55f-91c4b183e19d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running | run\n",
            "painting | paint\n",
            "walking | walk\n",
            "dressing | dress\n",
            "likely | like\n",
            "children | children\n",
            "whom | whom\n",
            "good | good\n",
            "ate | ate\n",
            "fishing | fish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using lemmatization in spacy\n",
        "doc = nlp(\"running painting walking dressing likely children who good ate fishing\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token, \" | \", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTuADJ0TedQC",
        "outputId": "d9358833-2f65-4428-f767-afbf03bae6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running  |  run\n",
            "painting  |  painting\n",
            "walking  |  walking\n",
            "dressing  |  dress\n",
            "likely  |  likely\n",
            "children  |  child\n",
            "who  |  who\n",
            "good  |  good\n",
            "ate  |  eat\n",
            "fishing  |  fish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DXho02vxfsFQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}